import { Link } from "@remix-run/react";

<div className='notes'>

## It's a plain `fetch` request

While OpenAI has a SDK for Node.js you can add, you can also make a raw `fetch` request to `https://api.openai.com/v1/chat/completions`

<div style={{ height: "50px" }} />
</div>

<CH.Scrollycoding>

1. Make a `POST` request to the OpenAI `chat/completions` endpoint.

```ts fetchChatCompletion.ts
fetch("https://api.openai.com/v1/chat/completions", {
  method: "POST",
});
```

---

2. Authorize the request with an API key.

```typescript focus=3:6 fetchChatCompletion.ts
fetch("https://api.openai.com/v1/chat/completions", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    // mark
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
  },
});
```

---

3a. The [`model`](https://platform.openai.com/docs/models/model-endpoint-compatibility) allows us to choose between things like gpt-3.5-turbo and gpt-4.

```typescript focus=7:15 fetchChatCompletion.ts
fetch("https://api.openai.com/v1/chat/completions", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
  },
  body: JSON.stringify({
    // mark(1)
    model: "gpt-3.5-turbo",
    messages: [
      {
        role: "user",
        content: message,
      },
    ],
  }),
});
```

---

3b. The `messages` array is how we pass in a chat history. The ChatGPT API itself has no memory/session state.

```typescript focus=7:15 fetchChatCompletion.ts
fetch("https://api.openai.com/v1/chat/completions", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
  },
  body: JSON.stringify({
    model: "gpt-3.5-turbo",
    // mark(1:6)
    messages: [
      {
        role: "user",
        content: message,
      },
    ],
  }),
});
```

</CH.Scrollycoding>

<div style={{ height: "75px" }} />

## `chat.completion` API Response

The answer will be in `choices[0].message.content`.

```json response.json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-3.5-turbo-0301",
  "choices": [
    {
      "message": {
        "role": "assistant",
        // mark(1)
        "content": "Hello there, how may I assist you today?"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 12,
    "total_tokens": 21
  }
}
```

<div style={{ height: "100px" }} />

<Link className="text-2xl font-bold" to="/talk/2/demo">
  Demo
</Link>

<div style={{ height: "50px" }} />
## Use it in Remix

<CH.Scrollycoding>

Make the OpenAI `fetch` request inside of a Remix action

- Protects the API key
- Return the `fetch` directly as the Action response

```ts focus=1:3,5[1:14] routes/demo.tsx
export const action = async ({ request }) => {
  let formData = await request.formData();
  let prompt = formData.get("prompt");

  return fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
    },
    body: JSON.stringify({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "user",
          content: prompt,
        },
      ],
    }),
  });
};
```

---

Submit to the `action` with a basic HTML `<form/>`.

```tsx routes/demo.tsx
export default function Demo() {
  return (
    <form method="post">
      <label>
        Prompt
        <textarea required name="prompt" />
      </label>
      <div>
        <button type="submit">Send</button>
      </div>
    </form>
  );
}
```

---

- We don't want to navigate after submission, we want to show the response.
- The trick is to use `fetcher.Form`

```tsx focus=2,5,13 routes/demo.tsx
export default function Demo() {
  let fetcher = useFetcher();

  return (
    <fetcher.Form method="post">
      <label>
        Prompt
        <textarea required name="prompt" />
      </label>
      <div>
        <button type="submit">Send</button>
      </div>
    </fetcher.Form>
  );
}
```

---

Display the API response stored on `fetcher.data`

```tsx focus=13:15 routes/demo.tsx
export default function Demo() {
  let fetcher = useFetcher();

  return (
    <fetcher.Form method="post">
      <label>
        Prompt
        <textarea required name="prompt" />
      </label>
      <div>
        <button type="submit">Send</button>
      </div>
      {fetcher.data && (
        <pre>{fetcher.data?.choices?.[0]?.message?.content}</pre>
      )}
    </fetcher.Form>
  );
}
```

</CH.Scrollycoding>

<div style={{ height: "300px" }} />

The final Remix route file would look like this

```tsx routes/demo.tsx
export default function Demo() {
  let fetcher = useFetcher();

  return (
    <fetcher.Form method="post">
      <label>
        Prompt
        <textarea required name="prompt" />
      </label>
      <div>
        <button type="submit">Send</button>
      </div>
      {fetcher.data && (
        <pre>{fetcher.data?.choices?.[0]?.message?.content}</pre>
      )}
    </fetcher.Form>
  );
}

export const action = async ({ request }) => {
  let formData = await request.formData();
  let prompt = formData.get("prompt");

  return fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
    },
    body: JSON.stringify({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "user",
          content: prompt,
        },
      ],
    }),
  });
};
```
